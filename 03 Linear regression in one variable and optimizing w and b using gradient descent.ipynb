{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86682e4",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2\"></a>\n",
    "# Problem Statement\n",
    "\n",
    "Let's use the same two data points as before - a house with 1000 square feet sold for \\\\$300,000 and a house with 2000 square feet sold for \\\\$500,000.\n",
    "\n",
    "| Size (1000 sqft)     | Price (1000s of dollars) |\n",
    "| ----------------| ------------------------ |\n",
    "| 1               | 300                      |\n",
    "| 2               | 500                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d80bde",
   "metadata": {},
   "source": [
    "Here we would not be finding values of w and b manually instead we would automate the process of optimizing  ùë§ and  ùëè using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1adfd49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math,copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a390e2",
   "metadata": {},
   "source": [
    "math is a built-in module in the Python 3 standard library that provides standard mathematical constants and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17833ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x_train=[1. 2.] \n",
      " y_train=[300. 500.]\n"
     ]
    }
   ],
   "source": [
    "x_train=np.array([1,2],dtype=float)\n",
    "y_train=np.array([300,500],dtype=float)\n",
    "print(f\" x_train={x_train} \\n y_train={y_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc9baf",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "we would need the cost function to find the best values of w and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95450df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(w,b,x,y):\n",
    "    m=x.shape[0]\n",
    "    cost_sum=0\n",
    "    for i in range(m):\n",
    "        f_wb=w*x[i]+b\n",
    "        cost=(f_wb-y[i])**2\n",
    "        cost_sum=cost_sum+cost\n",
    "    total_cost=cost_sum/(2*m)\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abec4ba",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "Now we will optimize the values of w and b using gradient descent algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985c817",
   "metadata": {},
   "source": [
    "Firstly we will define a compute_gradient function which will be finding the values of derivative terms and also we know that w and b must be simultaneously updated. \n",
    "\n",
    "*gradient descent* was described as:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w}   \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "where, parameters $w$, $b$ are updated simultaneously.  \n",
    "The gradient is defined as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here *simultaniously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a5805",
   "metadata": {},
   "source": [
    "Thus we see that deriavtive terms are first found before updating as the derivative terms create an impact in changing values of w and b in simultaneous update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9cdeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(w,b,x,y):\n",
    "    m=x.shape[0]\n",
    "    dj_dw=0\n",
    "    dj_db=0\n",
    "    for i in range(m):\n",
    "        f_wb=w*x[i]+b\n",
    "        dj_dw_i=(f_wb-y[i])*x[i]\n",
    "        dj_db_i=f_wb-y[i]\n",
    "        dj_dw=dj_dw+dj_dw_i\n",
    "        dj_db=dj_db+dj_db_i\n",
    "        \n",
    "    dj_dw=dj_dw/m\n",
    "    dj_db=dj_db/m\n",
    "    return dj_dw,dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf7db8",
   "metadata": {},
   "source": [
    "Now we can use the above functions to find the optimized values of w and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "787ecf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w,b,x,y,num_iters,alpha):\n",
    "    # A list to store cost J and w,b to show results\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    for i in range(num_iters):\n",
    "        dj_dw,dj_db=compute_gradient(w,b,x,y)\n",
    "        b=b-alpha*dj_db\n",
    "        w=w-alpha*dj_dw\n",
    "        \n",
    "#       now we will save the cost of each iteration along with the values of w and b\n",
    "        if i<100000:\n",
    "#       to prevent resource exhaustion\n",
    "            J_history.append(compute_cost(w,b,x,y))\n",
    "            p_history.append([w,b])\n",
    "        \n",
    "#       Now its obvious that we cannot display that many iterations with their respective costs and values of w and b ,\n",
    "#       and hence we would now Print cost at every intervals 10 times or as many iterations if < 10\n",
    "        \n",
    "#       math.ceil(x) is a function in math library used to round of the number to the higher bound\n",
    "#       eg: math.ceil(33.1)=34\n",
    "#       so here we are checking if i gives a remainder zero when divided by num_iters/10 those iterations cost will be printed\n",
    "        if i%math.ceil(num_iters/10)==0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "#       We used the f strings method of printing the digits with our required precision which we do same in C as %0.2f\n",
    "#       eg: {i:4} in fstring printing will print i upto 4 digits before decimal\n",
    "#       {J_history[-1]:0.2e} will print the last item appended in the total cost matrix of all terms \n",
    "#       and will print 2 digits after decimal\n",
    "#       e is used to represent number in scientific notation\n",
    "    return w,b,J_history,p_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1013a7",
   "metadata": {},
   "source": [
    "Now we will initalize w and b both to zero and run the gradient descent and also by providing the value of alpha and number of iterations to find the optimized value of w and b\n",
    "\n",
    "e is used in python to represent terms in scientific notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab675a76",
   "metadata": {},
   "source": [
    "Lets start with the lower learning rate (meaning lower value of alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "822afb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 7.93e+04  dj_dw: -6.500e+02, dj_db: -4.000e+02   w:  6.500e+00, b: 4.00000e+00\n",
      "Iteration 1000: Cost 3.41e+00  dj_dw: -3.712e-01, dj_db:  6.007e-01   w:  1.949e+02, b: 1.08228e+02\n",
      "Iteration 2000: Cost 7.93e-01  dj_dw: -1.789e-01, dj_db:  2.895e-01   w:  1.975e+02, b: 1.03966e+02\n",
      "Iteration 3000: Cost 1.84e-01  dj_dw: -8.625e-02, dj_db:  1.396e-01   w:  1.988e+02, b: 1.01912e+02\n",
      "Iteration 4000: Cost 4.28e-02  dj_dw: -4.158e-02, dj_db:  6.727e-02   w:  1.994e+02, b: 1.00922e+02\n",
      "Iteration 5000: Cost 9.95e-03  dj_dw: -2.004e-02, dj_db:  3.243e-02   w:  1.997e+02, b: 1.00444e+02\n",
      "Iteration 6000: Cost 2.31e-03  dj_dw: -9.660e-03, dj_db:  1.563e-02   w:  1.999e+02, b: 1.00214e+02\n",
      "Iteration 7000: Cost 5.37e-04  dj_dw: -4.657e-03, dj_db:  7.535e-03   w:  1.999e+02, b: 1.00103e+02\n",
      "Iteration 8000: Cost 1.25e-04  dj_dw: -2.245e-03, dj_db:  3.632e-03   w:  2.000e+02, b: 1.00050e+02\n",
      "Iteration 9000: Cost 2.90e-05  dj_dw: -1.082e-03, dj_db:  1.751e-03   w:  2.000e+02, b: 1.00024e+02\n",
      "\n",
      "\n",
      " cost J(w,b)=6.745014662580395e-06 with values of w=199.99285075131766 and b=100.011567727362\n",
      "\n",
      "\n",
      " cost J(w,b)=6.745014662580395e-06 with values of w=199.99285075131766 and b=100.011567727362\n"
     ]
    }
   ],
   "source": [
    "w_init=0\n",
    "b_init=0\n",
    "alpha=1.0e-2\n",
    "num_iters=10000\n",
    "w_final,b_final,J_hist,p_hist = gradient_descent(w_init,b_init,x_train,y_train,num_iters,alpha)\n",
    "print(f\"\\n\\n cost J(w,b)={min(J_hist)} with values of w={w_final} and b={b_final}\")\n",
    "print(f\"\\n\\n cost J(w,b)={J_hist[-1]} with values of w={w_final} and b={b_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a5b65",
   "metadata": {},
   "source": [
    "Thus we got the required minimum cost and optimized value of w and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f3ef1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 sqft house prediction 300.0 Thousand dollars\n",
      "1200 sqft house prediction 340.0 Thousand dollars\n",
      "2000 sqft house prediction 500.0 Thousand dollars\n"
     ]
    }
   ],
   "source": [
    "print(f\"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717613d7",
   "metadata": {},
   "source": [
    "Now, if we use large learning rate meaning larger value of alpha, then we can see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5554dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 2.58e+05  dj_dw: -6.500e+02, dj_db: -4.000e+02   w:  5.200e+02, b: 3.20000e+02\n",
      "Iteration    1: Cost 7.82e+05  dj_dw:  1.130e+03, dj_db:  7.000e+02   w: -3.840e+02, b:-2.40000e+02\n",
      "Iteration    2: Cost 2.37e+06  dj_dw: -1.970e+03, dj_db: -1.216e+03   w:  1.192e+03, b: 7.32800e+02\n",
      "Iteration    3: Cost 7.19e+06  dj_dw:  3.429e+03, dj_db:  2.121e+03   w: -1.551e+03, b:-9.63840e+02\n",
      "Iteration    4: Cost 2.18e+07  dj_dw: -5.974e+03, dj_db: -3.691e+03   w:  3.228e+03, b: 1.98886e+03\n",
      "Iteration    5: Cost 6.62e+07  dj_dw:  1.040e+04, dj_db:  6.431e+03   w: -5.095e+03, b:-3.15579e+03\n",
      "Iteration    6: Cost 2.01e+08  dj_dw: -1.812e+04, dj_db: -1.120e+04   w:  9.402e+03, b: 5.80237e+03\n",
      "Iteration    7: Cost 6.09e+08  dj_dw:  3.156e+04, dj_db:  1.950e+04   w: -1.584e+04, b:-9.80139e+03\n",
      "Iteration    8: Cost 1.85e+09  dj_dw: -5.496e+04, dj_db: -3.397e+04   w:  2.813e+04, b: 1.73730e+04\n",
      "Iteration    9: Cost 5.60e+09  dj_dw:  9.572e+04, dj_db:  5.916e+04   w: -4.845e+04, b:-2.99567e+04\n",
      "\n",
      "\n",
      " cost J(w,b)=257800.0 with values of w=-48453.651333120026 and b=-29956.671528960018\n",
      "\n",
      "\n",
      " cost J(w,b)=5604224211.962118 with values of w=-48453.651333120026 and b=-29956.671528960018\n"
     ]
    }
   ],
   "source": [
    "w_init=0\n",
    "b_init=0\n",
    "alpha=8.0e-1\n",
    "num_iters=10\n",
    "w_final,b_final,J_hist,p_hist = gradient_descent(w_init,b_init,x_train,y_train,num_iters,alpha)\n",
    "print(f\"\\n\\n cost J(w,b)={min(J_hist)} with values of w={w_final} and b={b_final}\")\n",
    "print(f\"\\n\\n cost J(w,b)={J_hist[-1]} with values of w={w_final} and b={b_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95800bf8",
   "metadata": {},
   "source": [
    "Using larger value of alpha gave us an extreme wrong values of w and b and missed the local minimum of cost function \n",
    "\n",
    "Hence we should always use small learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba241dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
